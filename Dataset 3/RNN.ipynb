{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGE9tVEYJDyY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, GlobalMaxPooling1D\n",
        "# from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Activation, Flatten, Input, concatenate, Conv1D, GlobalMaxPooling1D, MaxPooling1D,BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fs2svE4JfZT"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BndZpFFKJrB9"
      },
      "outputs": [],
      "source": [
        "link=\"https://drive.google.com/file/d/14dq1MXs4o0OO0bqTzEB2_qXqJcuvMh3Z/view?usp=sharing\"\n",
        "id = link.split(\"/\")[-2]\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('tweet_emotions.csv')\n",
        "df = pd.read_csv('tweet_emotions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyb0IfHaJ7SF",
        "outputId": "e0987c77-dc8a-45cf-cb76-a457a9c1eb8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.metrics.distance import jaccard_distance\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "\n",
        "correct_words = words.words()\n",
        "\n",
        "\n",
        "def spelling_fix(text):\n",
        "    new_text = []\n",
        "    incorrect_words = nltk.word_tokenize(text)\n",
        "    for word in incorrect_words:\n",
        "        temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
        "                                  set(ngrams(w, 2))),w)\n",
        "                for w in correct_words if w[0]==word[0]]\n",
        "        most_similar_list = sorted(temp, key = lambda val:val[0])\n",
        "\n",
        "        if len(most_similar_list) == 0:\n",
        "            new_text.append(word)\n",
        "            continue\n",
        "\n",
        "        if most_similar_list[0][0] >= 0.9:\n",
        "            new_text.append(most_similar_list[0][1])\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "\n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6LoOeJ7KGK9",
        "outputId": "c12ec9c8-8226-4ec8-ca11-b0bc7a7a044e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def remove_emoji(text):\n",
        "    regex_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    return regex_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def remove_email(text):\n",
        "    return re.sub('([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,})', '', text)\n",
        "\n",
        "\n",
        "def remove_repeated_char(text):\n",
        "    return re.sub(r'(.)\\1\\1{1,}', r'\\1\\1', text)\n",
        "\n",
        "\n",
        "def remove_account_tag(text):\n",
        "    return re.sub(r'@[\\w]+', '', text)\n",
        "\n",
        "\n",
        "def remove_hashtag(text):\n",
        "    return re.sub(r'#[\\w]+', '', text)\n",
        "\n",
        "\n",
        "def remove_links(text):\n",
        "    return re.sub(r'http[^\\s]+', '', text)\n",
        "\n",
        "\n",
        "def remove_spaces(text):\n",
        "    text = re.sub(r\"\\d+\", ' ', text)\n",
        "    text = re.sub(r\"\\n+\", ' ', text)\n",
        "    text = re.sub(r\"\\t+\", ' ', text)\n",
        "    text = re.sub(r\"\\r+\", ' ', text)\n",
        "    text = re.sub(r\"\\s+\", ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return \" \".join([w for w in word_tokens if not w in stop_words])\n",
        "\n",
        "\n",
        "def remove_less_2_characters(text):\n",
        "    return re.sub(r\"\\W*\\b\\w{1,2}\\b\", '', text)\n",
        "\n",
        "\n",
        "def lemmatize(text):\n",
        "    new_text = []\n",
        "    tokenization = nltk.word_tokenize(text)\n",
        "    for w in tokenization:\n",
        "        tmp_w = wordnet_lemmatizer.lemmatize(w)\n",
        "        if tmp_w is None:\n",
        "            new_text.append(w)\n",
        "        else:\n",
        "            new_text.append(tmp_w)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "\n",
        "def preprocess_text_sample(text):\n",
        "    text = remove_emoji(text)\n",
        "    text = remove_email(text)\n",
        "    text = remove_repeated_char(text)\n",
        "    text = remove_account_tag(text)\n",
        "    text = remove_hashtag(text)\n",
        "    text = remove_links(text)\n",
        "    text = remove_stop_words(text)\n",
        "    text = remove_spaces(text)\n",
        "    text = remove_less_2_characters(text)\n",
        "    text = text.strip()\n",
        "    text = text.lower()\n",
        "    #text = spelling_fix(text)\n",
        "    text = lemmatize(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_text_cols(df, col):\n",
        "    df[col] = df[col].apply(lambda x: preprocess_text_sample(x))\n",
        "    return df\n",
        "\n",
        "\n",
        "def preprocess_df(df, col=\"content\"):\n",
        "    df = preprocess_text_cols(df, col)\n",
        "    df.dropna(inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9IfD3f1Lsu2",
        "outputId": "b8132c01-18a0-4b34-c373-4b43749cbf70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "neutral       8638\n",
              "worry         8459\n",
              "happiness     5209\n",
              "sadness       5165\n",
              "love          3842\n",
              "surprise      2187\n",
              "fun           1776\n",
              "relief        1526\n",
              "hate          1323\n",
              "empty          827\n",
              "enthusiasm     759\n",
              "boredom        179\n",
              "anger          110\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"sentiment\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "l8Ox4-o7L0qM",
        "outputId": "d5ea41e5-5758-4612-e9f0-7c6f6cdd3854"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ab25f5ee3bde>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Set3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mcountplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot pass values for both `x` and `y`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2943\u001b[0;31m     plotter = _CountPlotter(\n\u001b[0m\u001b[1;32m   2944\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, errorbar, n_boot, units, seed, orient, color, palette, saturation, width, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1528\u001b[0m                  errcolor, errwidth, capsize, dodge):\n\u001b[1;32m   1529\u001b[0m         \u001b[0;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0m\u001b[1;32m   1531\u001b[0m                                  order, hue_order, units)\n\u001b[1;32m   1532\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;31m# Convert to a list of arrays, the common representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0;31m# The group names will just be numeric indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;31m# Convert to a list of arrays, the common representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0;31m# The group names will just be numeric indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    891\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    892\u001b[0m         \"\"\"\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'empty'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize = (12,6))\n",
        "sns.countplot(df[\"sentiment\"], palette='Set3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S7XdqGiODMv"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "def get_label_encoder_obj(y):\n",
        "    label_encoder = LabelEncoder()\n",
        "    return label_encoder.fit(y)\n",
        "\n",
        "\n",
        "def get_y_label_encoder(label_encoder, y):\n",
        "    return label_encoder.transform(y)\n",
        "\n",
        "\n",
        "def get_label_decoder(label_encoder, y):\n",
        "    return label_encoder.classes_[y]\n",
        "\n",
        "\n",
        "def one_hot_encode(y, num_classes):\n",
        "    return to_categorical(y, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og6b4eKgOLwO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, val = train_test_split(df, test_size=0.1, random_state=2, stratify=df[\"sentiment\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyMln-oROPwo"
      },
      "outputs": [],
      "source": [
        "label_encoder = get_label_encoder_obj(train[\"sentiment\"])\n",
        "train[\"sentiment\"] = get_y_label_encoder(label_encoder, train[\"sentiment\"])\n",
        "val[\"sentiment\"] = get_y_label_encoder(label_encoder, val[\"sentiment\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeMCA1XmOTrA",
        "outputId": "7594b3a2-c831-47a4-bde6-f735cb8774a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = len(np.unique(train[\"sentiment\"]))\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuQUvD6WOXYc"
      },
      "outputs": [],
      "source": [
        "X_train = train[\"content\"].values\n",
        "y_train = train[\"sentiment\"].values\n",
        "X_val = val[\"content\"].values\n",
        "y_val = val[\"sentiment\"].values\n",
        "X_test = val[\"content\"].values\n",
        "y_test = val[\"sentiment\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM8CUEpHOazb"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "y_val = le.transform(y_val)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_val = to_categorical(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uurrf7QvPlON"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "padding_type = 'post'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6AWfJdwPsJO"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M4MyYi9fM2n",
        "outputId": "99c5f0ec-d27c-4217-905b-15fd4b3993ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size = 45559\n"
          ]
        }
      ],
      "source": [
        "vocabSize = len(tokenizer.index_word) + 1\n",
        "print(f\"Vocabulary size = {vocabSize}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zlos8bZ6PvsH"
      },
      "outputs": [],
      "source": [
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "testing_sentences = tokenizer.texts_to_sequences(X_test)\n",
        "testing_padded = pad_sequences(testing_sentences, maxlen=max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9sF_LZIaihk"
      },
      "source": [
        "**RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvueVuKHak6T"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length)),\n",
        "model.add(SimpleRNN(128,activation='relu'))\n",
        "model.add (Dropout(0.3))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add (Dropout(0.3))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add (Dropout(0.3))\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add (Dropout(0.3))\n",
        "model.add(Dense(13))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjlY8t58atOD"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGmHJu3lawl7"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNYweb3DayoT"
      },
      "outputs": [],
      "source": [
        "training_labels_final = np.array(y_train)\n",
        "testing_labels_final = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ii6T_kpa4Tc"
      },
      "outputs": [],
      "source": [
        "history = model.fit(padded, training_labels_final, epochs=100, validation_data=(testing_padded, testing_labels_final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05Da4IyWa6Gj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
